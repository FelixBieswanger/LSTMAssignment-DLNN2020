{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "\"\"\"\n",
                "Minimal character-level LSTM model. Written by Ngoc Quan Pham\n",
                "Code structure borrowed from the Vanilla RNN model from Andreij Karparthy @karparthy.\n",
                "BSD License\n",
                "\"\"\"\n",
                "import numpy as np\n",
                "from random import uniform\n",
                "import sys\n",
                "\n",
                "\n",
                "def sigmoid(x):\n",
                "    return 1 / (1 + np.exp(-x))\n",
                "\n",
                "\n",
                "def dsigmoid(y):\n",
                "    return y * (1 - y)\n",
                "\n",
                "\n",
                "def dtanh(x):\n",
                "    return 1 - x * x\n",
                "\n",
                "\n",
                "# The numerically stable softmax implementation\n",
                "def softmax(x):\n",
                "    # assuming x shape is [feature_size, batch_size]\n",
                "    e_x = np.exp(x - np.max(x, axis=0))\n",
                "    return e_x / e_x.sum(axis=0)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Look at Setup"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "# data I/O\n",
                "data = open('data/input.txt', 'r').read()  # should be simple plain text file\n",
                "chars = sorted(list(set(data))) #unique chars in input.txt\n",
                "data_size, vocab_size = len(data), len(chars)\n",
                "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
                "char_to_ix = {ch: i for i, ch in enumerate(chars)} # dict with char key and index value\n",
                "ix_to_char = {i: ch for i, ch in enumerate(chars)} # visa versa\n",
                "std = 0.1\n",
                "\n",
                "# hyperparameters\n",
                "emb_size = 16\n",
                "hidden_size = 256  # size of hidden layer of neurons\n",
                "seq_length = 128  # number of steps to unroll the RNN for\n",
                "learning_rate = 5e-2\n",
                "max_updates = 500000\n",
                "batch_size = 32\n",
                "concat_size = emb_size + hidden_size\n",
                "\n",
                "\n",
                "# model parameters\n",
                "# char embedding parameters\n",
                "Wex = np.random.randn(emb_size, vocab_size) * std  # embedding layer\n",
                "\n",
                "# LSTM parameters\n",
                "Wf = np.random.randn(hidden_size, concat_size) * std  # forget gate\n",
                "Wi = np.random.randn(hidden_size, concat_size) * std  # input gate\n",
                "Wo = np.random.randn(hidden_size, concat_size) * std  # output gate\n",
                "Wc = np.random.randn(hidden_size, concat_size) * std  # c term\n",
                "\n",
                "bf = np.zeros((hidden_size, 1))  # forget bias\n",
                "bi = np.zeros((hidden_size, 1))  # input bias\n",
                "bo = np.zeros((hidden_size, 1))  # output bias\n",
                "bc = np.zeros((hidden_size, 1))  # memory bias\n",
                "\n",
                "# Output layer parameters\n",
                "Why = np.random.randn(vocab_size, hidden_size) * std  # hidden to output\n",
                "by = np.random.randn(vocab_size, 1) * std  # output bias\n",
                "\n",
                "# all chars in data as corresponding index\n",
                "# one vektor 1115394 x 1\n",
                "data_stream = np.asarray([char_to_ix[char] for char in data]) \n",
                "\n",
                "bound = (data_stream.shape[0] // (seq_length *\n",
                "                                  batch_size)) * (seq_length * batch_size)\n",
                "cut_stream = data_stream[:bound]\n",
                "\n",
                "# transform big vektor to smaller vektors according to batch size\n",
                "cut_stream = np.reshape(cut_stream, (batch_size, -1))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "data has 1115394 characters, 65 unique.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Look at Forward Pass"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.4",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.4 64-bit"
        },
        "interpreter": {
            "hash": "7e85ec9bf098c5427e45e2f632dcd4eeff803b007e1abd287d600879388709c1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}